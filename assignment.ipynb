{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919d67f9-5a30-4eb8-be92-8e67482cdc3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Acquisition and Processing Systems (DaPS) (ELEC0136)    \n",
    "### Final Assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b0021-1259-41ab-bdbb-c076828cd77f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "\n",
    "#### Task 1: Data Acquisition\n",
    "\n",
    "You will first have to acquire the necessary data for conducting your study. One essential type of\n",
    "data that you will need, are the stock prices for each company from April 2017 to April 202 1 as\n",
    "described in Section 1. Since these companies are public, the data is made available online. The\n",
    "first task is for you to search and collect this data, finding the best way to access and download\n",
    "it. A good place to look is on platforms that provide free data relating to the stock market such as\n",
    "Google Finance or Yahoo! Finance.\n",
    "\n",
    "[Optional] Providing more than one method to acquire the very same or different data, e.g. from\n",
    "a downloaded comma-separated-value file and a web API, will result in a higher score.\n",
    "\n",
    "There are many valuable sources of information for analysing the stock market. In addition to time\n",
    "series depicting the evolution of stock prices, acquire auxiliary data that is likely to be useful for\n",
    "the forecast, such as:\n",
    "\n",
    "- Social Media, e.g., Twitter: This can be used to uncover the public’s sentimental\n",
    "response to the stock market\n",
    "- Financial reports: This can help explain what kind of factors are likely to affect the stock\n",
    "market the most\n",
    "- News: This can be used to draw links between current affairs and the stock market\n",
    "- Climate data: Sometimes weather data is directly correlated to some companies’ stock\n",
    "prices and should therefore be taken into account in financial analysis\n",
    "- Others: anything that can justifiably support your analysis.\n",
    "\n",
    "Remember, you are looking for historical data, not live data.\n",
    "   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c614bfb6-cdf2-414c-9dee-ea8a8db97de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire():\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    import csv\n",
    "    import urllib\n",
    "\n",
    "    import pandas_datareader.data as web\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    def download_save_csv(url,filename):\n",
    "        \"\"\" To download and save .csv file according to url and save path\"\"\"\n",
    "        data_dir='./Dataset'\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)    \n",
    "        file_path=data_dir+'/'+filename+'.csv'\n",
    "        if not os.path.exists(file_path):\n",
    "            file_path, headers = urllib.request.urlretrieve(url,file_path)   \n",
    "        return file_path\n",
    "\n",
    "    stock_url = 'https://query1.finance.yahoo.com/v7/finance/download/AAL?period1=1491004800&period2=1619827200&interval=1d&events=history&includeAdjustedClose=true'\n",
    "    stock_Path=download_save_csv(stock_url,'stock')\n",
    "    stock=pd.read_csv(stock_Path,parse_dates=True) \n",
    "\n",
    "    COVID_url = \"https://covid19.who.int/WHO-COVID-19-global-data.csv\"\n",
    "    Cases_Path= download_save_csv(COVID_url,'cases')\n",
    "    Cases=pd.read_csv(Cases_Path,parse_dates=True)  \n",
    "    caseUS=Cases.loc[Cases[\"Country_code\"] == \"US\"]\n",
    "    Cases.shape[0]\n",
    "    Cases.dtypes\n",
    "    Precipitation_url = \"https://www.ncdc.noaa.gov/cag/national/time-series/110-pcp-all-12-2017-2021.csv?base_prd=true&begbaseyear=2017&endbaseyear=2021\"\n",
    "    Precip_Path = download_save_csv(Precipitation_url ,'Precip')\n",
    "    Precip=pd.read_csv(Precip_Path,skiprows=[0,1,2,3], parse_dates=True,infer_datetime_format=True,date_parser=True)\n",
    "    Precip.shape[0]\n",
    "    Precip.dtypes\n",
    "    \n",
    "    startDate=datetime.datetime(2017,4,1)\n",
    "    endDate=datetime.datetime(2021,5,31)\n",
    "    Stock =web.DataReader(\"AAL\",\"yahoo\",startDate,endDate)\n",
    "    Stock.to_csv('./Dataset/Stock.csv',index=True, header=True)\n",
    "\n",
    "    startDate=datetime.datetime(2017,4,1)\n",
    "    endDate=datetime.datetime(2021,5,31)\n",
    "    Oil =web.DataReader(\"CL=F\",\"yahoo\",startDate,endDate)\n",
    "    Oil.to_csv('./Dataset/Oil.csv',index=True, header=True) \n",
    "\n",
    "    startDate=datetime.datetime(2017,4,1)\n",
    "    endDate=datetime.datetime(2021,5,31)\n",
    "    NASDAQ =web.DataReader(\"^IXIC\",\"yahoo\",startDate,endDate)\n",
    "    NASDAQ.to_csv('./Dataset/NASDAQ.csv',index=True, header=True) \n",
    "    print('task1 succeed')\n",
    "    return Stock,Oil, NASDAQ,caseUS,Precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd303d-9419-4993-a2b3-fe143bf2d954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26ea523a-6d00-46db-873b-476600070902",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "    \n",
    "## Task 2: Data Storage\n",
    "\n",
    "Once you have found a way to acquire the relevant data, you need to decide on how to store it.\n",
    "You should choose a format that allows an efficient read access to allow training a parametric\n",
    "model. Also, the data corpus should be such that it can be easily inspected. Data can be stored\n",
    "locally, on your computer.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224e0f8c-da42-4b91-a696-3c1f5d52ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(Stock,Oil, NASDAQ,caseUS,Precip):\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    import csv\n",
    "    import urllib\n",
    "    import pandas_datareader.data as web\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    import matplotlib\n",
    "    import pymongo\n",
    "    def save_to_db(dataName,Name):\n",
    "        \"\"\"Transform Dataframe to dictionary then save then to MongoDB\"\"\"\n",
    "        if not dataName.count_documents({}, limit = 1):\n",
    "            dataName.insert_many(Name.to_dict(orient='records'))\n",
    "        else:\n",
    "            dataName.delete_many({})\n",
    "            dataName.insert_many(Name.to_dict(orient='records'))\n",
    "\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://hcy:0523@cluster0.piobu.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "    db = client.Assign\n",
    "    StockPrices = db['StockPrices']\n",
    "    Infects = db['Infects']\n",
    "    Precip_db = db['Precip_db']\n",
    "    Oil_db = db['Oil_db']\n",
    "    NASDAQ_db = db['NASDAQ_db']\n",
    "\n",
    "    Stock.reset_index(inplace=True)\n",
    "    caseUS.reset_index(inplace=True)\n",
    "    Precip.reset_index(inplace=True)\n",
    "    Oil.reset_index(inplace=True)\n",
    "    NASDAQ.reset_index(inplace=True)\n",
    "\n",
    "    save_to_db(StockPrices,Stock)\n",
    "    save_to_db(Infects,caseUS)\n",
    "    save_to_db(Precip_db,Precip)\n",
    "    save_to_db(Oil_db,Oil)\n",
    "    save_to_db(NASDAQ_db,NASDAQ)\n",
    "\n",
    "    client.close()\n",
    "    print('task2 succeed')\n",
    "    return StockPrices,Oil_db,NASDAQ_db, Infects, Precip_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f354e27-4e73-49db-871d-8f7319376fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0746eb74-c388-4722-a0b8-b7a81f4a4182",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-warning\">\n",
    "\n",
    "[Optional] Create a simple API to allow Al retrieving the compound of data you collected. It is enough to provide a single access point to retrieve all the data, and not implement query mechanism. The API must be accessible from the web. If you engage in this task data must be stored online.  \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3882fe99-4d00-4367-8a1c-48592c16d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(dataName):\n",
    "    \"\"\"retrieve dataset from MongoDB\"\"\"\n",
    "    import pymongo\n",
    "    import pandas as pd\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://hcy:0523@cluster0.piobu.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "    db = client.Assign\n",
    "    if dataName.count_documents({}, limit = 1):\n",
    "        table = dataName.find()\n",
    "        Data=pd.DataFrame.from_records(table)\n",
    "        Data.drop('_id',axis = 1,inplace = True)\n",
    "        client.close()\n",
    "        return Data\n",
    "\n",
    "    else:\n",
    "        client.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f775e-173b-4a16-b46e-0a8265a0eaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc93fdd1-7c24-403c-b36f-66906d29ebaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "\n",
    "## Task 3: Data Preprocessing\n",
    "\n",
    "Now that you have the data stored, you can start preprocessing it. Think about what features to\n",
    "keep, which ones to transform, combine or discard. Make sure your data is clean and consistent\n",
    "(e.g., are there many outliers? any missing values?). You are expected to:\n",
    "\n",
    "1. Clean the data from missing values and outliers, if any.\n",
    "2. Provide useful visualisation of the data. Plots should be saved on disk, and not printed on\n",
    "the juptyer notebook.\n",
    "3. Transform your data (e.g., using normalization, dimensionality reduction, etc.) to improve\n",
    "the forecasting performance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5458b2-98ca-4301-8a58-7309b6e72cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(StockPrices,Oil_db,NASDAQ_db, Infects, Precip_db):\n",
    "    \"\"\"\n",
    "    Data Preprocess\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    import csv\n",
    "    import urllib\n",
    "\n",
    "    import pandas_datareader.data as web\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    import matplotlib\n",
    "    import pymongo\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy import stats\n",
    "    import seaborn as sn\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \"\"\"retrieve data from MongoDB\"\"\"\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://hcy:0523@cluster0.piobu.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "    db = client.Assign\n",
    "    Stock_data=retrieve(db.StockPrices)\n",
    "    Cases_data=retrieve(db.Infects)\n",
    "    Precip_data=retrieve(db.Precip_db)\n",
    "    Oil_data=retrieve(db.Oil_db)\n",
    "    NASDAQ_data=retrieve(db.NASDAQ_db)\n",
    "    \"\"\"\n",
    "    Data Choosing,only figure uout useful Dataset\n",
    "    \"\"\"\n",
    "    Stock_data.rename(columns={'Close':'Stock_Price'},inplace=True)\n",
    "    Stock_data=Stock_data.loc[:,['Date','Stock_Price']]\n",
    "    Stock_data.set_index('Date',drop=True,inplace=True)\n",
    "\n",
    "    Oil_data.rename(columns={'Close':'Oil_Price'},inplace=True)\n",
    "    Oil_data=Oil_data.loc[:,['Date','Oil_Price']]\n",
    "    Oil_data.set_index('Date',drop=True,inplace=True)\n",
    "\n",
    "    NASDAQ_data.rename(columns={'Close':'NASDAQ_Index'},inplace=True)\n",
    "    NASDAQ_data=NASDAQ_data.loc[:,['Date','NASDAQ_Index']]\n",
    "    NASDAQ_data.set_index('Date',drop=True,inplace=True)\n",
    "\n",
    "    Precip_data.set_index('Date',inplace=True)\n",
    "    Precip_data.rename(columns={'Value':'Precipitation'},inplace=True)\n",
    "    Precip_data=Precip_data.loc[:,['Precipitation']]\n",
    "\n",
    "    Cases_data=Cases_data.loc[Cases_data[\"Date_reported\"] < \"2021-12-31\"]\n",
    "    Cases_data=Cases_data.rename(columns={'Date_reported':'Date'})\n",
    "    Cases_data=Cases_data[['Date','New_cases']]\n",
    "    Cases_data.set_index('Date',drop=True,inplace=True)\n",
    "    Cases_data['New_cases']=True\n",
    "    image_dir='./image/Data Preprocessing/outliers'\n",
    "    \n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)   \n",
    "\n",
    "    def Z_score(Data,yTitle,filename):\n",
    "        \"\"\"\n",
    "        Using Z-Score to figure out outliers\n",
    "        Then use Cap method to alter outliers\n",
    "        \"\"\"\n",
    "        day=Data.reset_index()\n",
    "        data=np.array(Data).reshape(1,len(day))[0]\n",
    "        day=np.array(day.iloc[:,0])\n",
    "        day=day.reshape(1,len(day))[0]\n",
    "        z = np.abs(stats.zscore(data))\n",
    "        threshold = 3\n",
    "        loc = np.where(z > threshold)\n",
    "        outlier = data[loc]\n",
    "\n",
    "        drop = np.array([remain for remain in data if remain not in outlier])\n",
    "        drop_day = np.array([remain for remain in day if remain not in day[loc]])\n",
    "        cap=np.copy(data)\n",
    "        # cap the outliers\n",
    "        Num=[]\n",
    "        for i,element in enumerate(cap[loc]):\n",
    "            if element>np.mean(drop):\n",
    "                Num.append(np.max(drop))\n",
    "            else:\n",
    "                Num.append(np.min(drop))\n",
    "\n",
    "        cap[loc]=Num\n",
    "        Data.iloc[:,0]=cap \n",
    "\n",
    "        file_path=image_dir+'/'+filename+'.jpg'\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, sharey=True,sharex=True, figsize=(10, 16))\n",
    "        ax1.set_title('Before Cap')\n",
    "        ax1.scatter(day,data)\n",
    "        ax1.scatter(day[loc], data[loc], c='r')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel(yTitle)\n",
    "        ax1.grid(which='major',axis='y')\n",
    "\n",
    "        ax2.set_title('After cap')\n",
    "        ax2.scatter(day,cap)\n",
    "        ax2.scatter(day[loc], cap[loc], c='r')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel(yTitle)\n",
    "        ax2.grid(which='major',axis='y')\n",
    "        plt.savefig(file_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def  IQR_score(outlier_dataset):\n",
    "        \"\"\"\n",
    "        Just a test of IQR-Score, it is not used in programmed\"\"\"\n",
    "        outlier_dataset=np.array(outlier_dataset)\n",
    "        Q1 = np.quantile(outlier_dataset,0.25)\n",
    "        Q3 = np.quantile(outlier_dataset,0.75)\n",
    "        IQR = Q3-Q1\n",
    "        Minimum = Q1-1.5*IQR\n",
    "        Maximum = Q3+1.5*IQR\n",
    "\n",
    "        outlier_by_IQR_Score=outlier_dataset[(outlier_dataset<Minimum) | (outlier_dataset>Maximum)]   \n",
    "\n",
    "    plt.boxplot(Stock_data)\n",
    "    plt.savefig('./image/Data Preprocessing/outliers/boxStock')\n",
    "    Z_score(Stock_data,\"Price in USD\",\"Stock\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.boxplot(Oil_data)\n",
    "    plt.savefig('./image/Data Preprocessing/outliers/boxOil')\n",
    "    Z_score(Oil_data,\"prices in USD\",\"Oil\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.boxplot(NASDAQ_data)\n",
    "    plt.savefig('./image/Data Preprocessing/outliers/boxNASDAQ')\n",
    "    Z_score(NASDAQ_data,\"prices in USD\",'NASDAQ')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.boxplot(Precip_data)\n",
    "    plt.savefig('./image/Data Preprocessing/outliers/boxPrecip')\n",
    "    Z_score(Precip_data,\"inch\",'Precipiation')\n",
    "    plt.close()\n",
    "\n",
    "    image_dir='./image/Data Preprocessing/missing'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)  \n",
    "    def Resample(df):\n",
    "        \"\"\"resample and interpolate monthly precipitation\n",
    "        \"\"\"\n",
    "\n",
    "        df_RE = df.resample('1D')\n",
    "\n",
    "        df_RE = df_RE.interpolate(method='time')\n",
    "\n",
    "\n",
    "        return df_RE\n",
    "    Precip_data.reset_index(inplace=True)\n",
    "    Precip_data['Date']=[datetime.datetime.strptime(str(time),'%Y%m') for time in Precip_data['Date']]\n",
    "    Precip_data.set_index('Date',inplace=True,drop=True)\n",
    "    Precip_data=Resample(Precip_data)\n",
    "    Cases_data.reset_index(inplace=True)\n",
    "    Cases_data.set_index('Date',inplace=True,drop=True)\n",
    "    Cases_data.index=pd.DatetimeIndex(Cases_data.index)\n",
    "#     Combine.isnull().sum() \n",
    "    Combine = pd.concat([Stock_data,Oil_data,NASDAQ_data,Precip_data,Cases_data],axis=1)\n",
    "    Combine.dropna(axis=0,subset = ['Stock_Price'],inplace=True)\n",
    "    Combine['New_cases'].fillna(False,inplace=True)    \n",
    "    print('Wheher the missing value exists:')\n",
    "    print(Combine.isnull().any())\n",
    "    Combine.to_csv('./Dataset/Combine.csv', index=True, header=True)\n",
    "#     print(Combine.columns)\n",
    "#     print(Combine.index)\n",
    "    Y = Combine.loc[:,'Stock_Price']\n",
    "    Y1 = Combine.loc[:,'Oil_Price']\n",
    "    Y2 = Combine.loc[:,'NASDAQ_Index']\n",
    "    Y3 = Combine.loc[:,'New_cases']\n",
    "    Y4 = Combine.loc[:,'Precipitation']\n",
    "    \"\"\"\n",
    "    Plot the data after process outliers and missing values\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharey=True, figsize=(14, 14))\n",
    "    ax1.hist(Y)\n",
    "    ax1.set_ylabel('Stock_Price Count')\n",
    "    ax2.hist(Y1)\n",
    "    ax2.set_ylabel('Oil_Price Count')\n",
    "    ax3.hist(Y2)\n",
    "    ax3.set_ylabel('NASDAQ_Index Count')\n",
    "    ax4.hist(Y4)\n",
    "    ax4.set_ylabel('Precipitation Count')\n",
    "    plt.savefig('./image/Data Preprocessing/missing/hist')\n",
    "    plt.close()  \n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) =plt.subplots(5, sharex=True,figsize=(10, 16))\n",
    "    ax1.plot(Y,label='Stock_Price', c='r')\n",
    "    ax1.set_ylabel('Stock Price in USD')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(Y1,label='Oil_Price', c='b')\n",
    "    ax2.set_ylabel('Oil Price in USD')\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3.plot(Y2,label='NASDAQ_Index', c='y')\n",
    "    ax3.set_ylabel('NASDAQ Index')\n",
    "    ax3.grid( )\n",
    "    ax3.legend()\n",
    "\n",
    "    ax4.plot(Y3,label='New_cases', c='g')\n",
    "    ax4.set_ylabel('New COVID cases in US')\n",
    "    ax4.grid()\n",
    "    ax4.legend()\n",
    "\n",
    "    ax5.plot(Y4,label='Precipitation', c='black')\n",
    "    ax5.set_xlabel('Date')\n",
    "    ax5.set_ylabel('Precipitation in Inches')\n",
    "    ax5.grid()\n",
    "    ax5.legend()\n",
    "\n",
    "    plt.savefig('./image/Data Preprocessing/missing/plot')\n",
    "    plt.close()\n",
    "    Precip_data['Precipitation']=Combine['Precipitation']\n",
    "    Precip_data.dropna(axis=0,inplace=True)\n",
    "    Combine_fit=Combine.copy()\n",
    "    \"\"\"Normalized data to [0,1] scale.\"\"\"\n",
    "    scaler0 = MinMaxScaler()\n",
    "    Stock_fit=Stock_data.copy()\n",
    "    Stock_fit['Stock_Price']=scaler0.fit_transform(Stock_data)\n",
    "\n",
    "    scaler1 = MinMaxScaler()\n",
    "    Oil_fit=Oil_data.copy()\n",
    "    Oil_fit['Oil_Price']=scaler1.fit_transform(Oil_data)\n",
    "\n",
    "    scaler2 = MinMaxScaler()\n",
    "    NASDAQ_fit=NASDAQ_data.copy()\n",
    "    NASDAQ_fit['NASDAQ_Index']=scaler2.fit_transform(NASDAQ_data)\n",
    "\n",
    "    scaler3 = MinMaxScaler()\n",
    "    Precip_fit=Precip_data.copy()\n",
    "    Precip_fit['Precipitation']=scaler3.fit_transform(Precip_data)\n",
    "    X = Combine_fit['Stock_Price']\n",
    "    X1 = Combine_fit['Oil_Price']\n",
    "    X2 = Combine_fit['NASDAQ_Index']\n",
    "    X4 = Combine_fit['Precipitation']\n",
    "    Combine_fit=Combine.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    Combine_fit=scaler.fit_transform(Combine)\n",
    "    Combine_fit=pd.DataFrame(Combine_fit,index=Combine.index,columns=Combine.columns)\n",
    "    Combine_fit.boxplot()\n",
    "    plt.savefig('./image/Data Preprocessing/missing/box_fit')\n",
    "    plt.close()\n",
    "    print('task3 succeed')\n",
    "    return Stock_data,Oil_data, NASDAQ_data,Precip_data,Combine,Stock_fit,Oil_fit,NASDAQ_fit,Precip_fit,Combine_fit,scaler0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6fc43-5ac4-429d-9f42-ee77cc8d56a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "166a02fb-ad30-4ac8-b45a-16ba63299839",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "    \n",
    "## Task 4: Data Exploration\n",
    "\n",
    "After ensuring that the data is well preprocessed, it is time to start exploring the data to carry out\n",
    "hypotheses and intuition about possible patterns that might be inferred. Depending on the data,\n",
    "different EDA (exploratory data analysis) techniques can be applied, and a large amount of\n",
    "information can be extracted.\n",
    "For example, you could do the following analysis:\n",
    "\n",
    "    \n",
    "- Time series data is normally a combination of several components:\n",
    "  - Trend represents the overall tendency of the data to increase or decrease over time.\n",
    "  - Seasonality is related to the presence of recurrent patterns that appear after regular\n",
    "intervals (like seasons).\n",
    "  - Random noise is often hard to explain and represents all those changes in the data\n",
    "that seem unexpected. Sometimes sudden changes are related to fixed or predictable\n",
    "events (i.e., public holidays).\n",
    "- Features correlation provides additional insight into the data structure. Scatter plots and\n",
    "boxplots are useful tools to spot relevant information.\n",
    "- Explain unusual behaviour.\n",
    "- Explore the correlation between stock price data and other external data that you can\n",
    "collect (as listed in Sec 2.1)\n",
    "- Use hypothesis testing to better understand the composition of your dataset and its\n",
    "representativeness.\n",
    "\n",
    "    \n",
    "At the end of this step, provide key insights on the data. This data exploration procedure should\n",
    "inform the subsequent data analysis/inference procedure, allowing one to establish a predictive\n",
    "relationship between variables.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191b9ba6-7e5f-47dd-ba43-902b0972e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(Stock_data,Oil_data, NASDAQ_data,Precip_data,Combine,Stock_fit,Oil_fit,NASDAQ_fit,Precip_fit,Combine_fit):\n",
    "    \"\"\"\n",
    "    Data Exploration\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    import matplotlib\n",
    "    import pymongo\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    from scipy import stats\n",
    "    import seaborn as sn\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from scipy.stats import pearsonr,spearmanr,chi2, chi2_contingency\n",
    "    from calendar import day_abbr, month_abbr, mdays\n",
    "    import holidays\n",
    "\n",
    "    image_dir='./image/Data Exploration'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    Des=Combine.describe()\n",
    "    Des.to_csv('./Dataset/Combine_describe.csv',index=True, header=True) \n",
    "\n",
    "    def seasonal_cycle(Dataset,title):\n",
    "        \"\"\"\n",
    "        Explorae the seasonlity of Dataset\n",
    "        \"\"\"\n",
    "        seasonal_cycle = Dataset.rolling(window=30, center=True).mean().groupby(Dataset.index.dayofyear).mean()\n",
    "        q25 = Dataset.rolling(window=30, center=True).mean().groupby(Dataset.index.dayofyear).quantile(0.25)\n",
    "        q75 = Dataset.rolling(window=30, center=True).mean().groupby(Dataset.index.dayofyear).quantile(0.75)\n",
    "        Day_in_Mon = mdays.copy()\n",
    "        Day_in_Mon[2] = 29\n",
    "        Day_in_Mon = np.cumsum(Day_in_Mon)\n",
    "        month_ticks = month_abbr[1:]\n",
    "        Fre, axes = plt.subplots(figsize=(10,7)) \n",
    "\n",
    "        seasonal_cycle.plot(ax=axes, lw=2, color='b', legend=False)\n",
    "        axes.fill_between(seasonal_cycle.index, q25.values.ravel(), q75.values.ravel(), color='b', alpha=0.3)\n",
    "        axes.set_xticklabels(month_ticks)\n",
    "        axes.grid(ls=':')\n",
    "        axes.set_xlabel('Month', fontsize=15)\n",
    "        axes.set_ylabel(title, fontsize=15);\n",
    "        axes.set_xlim(0, 365)\n",
    "        [l.set_fontsize(13) for l in axes.xaxis.get_ticklabels()]\n",
    "        [l.set_fontsize(13) for l in axes.yaxis.get_ticklabels()]\n",
    "\n",
    "        axes.set_title('30 days running average '+title, fontsize=15)\n",
    "        plt.savefig('./image/Data Exploration/seasonal_cycle '+title)\n",
    "        plt.close()\n",
    "\n",
    "    seasonal_cycle(Stock_data,'Price of stock')\n",
    "    seasonal_cycle(Oil_data,'Price of Oil')\n",
    "    seasonal_cycle(NASDAQ_data,'NASDAQ Index')\n",
    "    seasonal_cycle(Precip_data,'Precipitation')\n",
    "\n",
    "    def year_and_month(Dataset,title):\n",
    "        \"\"\"\n",
    "        Explorae the year and month regularity of Dataset\n",
    "        \"\"\"\n",
    "        month_year = Dataset.copy()\n",
    "        month_year.loc[:,'year'] = month_year.index.year\n",
    "        month_year.loc[:,'month'] = month_year.index.month\n",
    "        month_year = month_year.groupby(['year','month']).mean().unstack()\n",
    "        month_year.columns = month_year.columns.droplevel(0)\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "        sn.heatmap(month_year, ax=ax, cmap=plt.cm.viridis, cbar_kws={'boundaries':np.arange(10000,45000,5000)})\n",
    "\n",
    "        cbax = f.axes[1]\n",
    "        [l.set_fontsize(13) for l in cbax.yaxis.get_ticklabels()]\n",
    "        cbax.set_ylabel(title, fontsize=13)\n",
    "\n",
    "        [ax.axhline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 7)]\n",
    "        [ax.axvline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 24)];\n",
    "\n",
    "        ax.set_title(title+' per year and month', fontsize=16)\n",
    "\n",
    "        [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "        [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "        ax.set_xlabel('Month', fontsize=15)\n",
    "        ax.set_ylabel('Year', fontsize=15)\n",
    "        ax.set_yticklabels(np.arange(2017, 2022, 1), rotation=0);\n",
    "        plt.savefig('./image/Data Exploration/year_and_month '+title)\n",
    "        plt.close()\n",
    "\n",
    "    year_and_month(Stock_data,'Price of stock')\n",
    "    year_and_month(Oil_data,'Price of Oil')\n",
    "    year_and_month(NASDAQ_data,'NASDAQ Index')\n",
    "    year_and_month(Precip_data,'Precipitation')\n",
    "\n",
    "    def month_day(Dataset,title):\n",
    "        \"\"\"\n",
    "        Explorae the day of the week and month regularity of Dataset\n",
    "        \"\"\"\n",
    "        month_day = Dataset.copy()\n",
    "        month_day.loc[:,'day_of_week'] = month_day.index.dayofweek\n",
    "        month_day.loc[:,'month'] = month_day.index.month\n",
    "        month_day = month_day.groupby(['day_of_week','month']).mean().unstack()\n",
    "        month_day.columns = month_day.columns.droplevel(0)\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "        sn.heatmap(month_day, ax = ax, cmap=plt.cm.viridis, cbar_kws={'boundaries':np.arange(10000,45000,5000)})\n",
    "\n",
    "        cbax = f.axes[1]\n",
    "        [l.set_fontsize(13) for l in cbax.yaxis.get_ticklabels()]\n",
    "        cbax.set_ylabel('Santander cycles hires', fontsize=13)\n",
    "\n",
    "        [ax.axhline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 7)]\n",
    "        [ax.axvline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 24)];\n",
    "\n",
    "        ax.set_title(title+' per day of the week and month', fontsize=16)\n",
    "\n",
    "        [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "        [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "        ax.set_xlabel('Month', fontsize=15)\n",
    "        ax.set_ylabel('Day of the week', fontsize=15)\n",
    "        ax.set_yticklabels(day_abbr[0:month_day.shape[0]]);\n",
    "        plt.savefig('./image/Data Exploration/month_day '+title)\n",
    "        plt.close()\n",
    "\n",
    "    month_day(Stock_data,'Price of stock')\n",
    "    month_day(Oil_data,'Price of Oil')\n",
    "    month_day(NASDAQ_data,'NASDAQ Index')\n",
    "    month_day(Precip_data,'Precipitation')\n",
    "\n",
    "    holidays_df = pd.DataFrame([], columns = ['ds','holiday'])\n",
    "    ldates = []\n",
    "    lnames = []\n",
    "    for date, name in sorted(holidays.UnitedStates(years=np.arange(2017, 2021 + 1)).items()):\n",
    "        ldates.append(date)\n",
    "        lnames.append(name)\n",
    "    \"\"\"\n",
    "    Filter the date in dataset which are also holidays\n",
    "    \"\"\"\n",
    "    ldates = np.array(ldates)\n",
    "    lnames = np.array(lnames)\n",
    "    holidays_df.loc[:,'ds'] = ldates\n",
    "    holidays_df.loc[:,'holiday'] = lnames\n",
    "    holidays_df.holiday.unique()\n",
    "    holidays_df.loc[:,'holiday'] = holidays_df.loc[:,'holiday'].apply(lambda x : x.replace(' (Observed)',''))\n",
    "\n",
    "    Dataset = Stock_data.copy()\n",
    "    holidays = Dataset.loc[Dataset.index.isin(holidays_df['ds'])]\n",
    "    normaldays = Dataset.loc[~Dataset.index.isin(holidays_df['ds'])]\n",
    "    summary_month_holidays = holidays.groupby(holidays.index.month).describe()\n",
    "    summary_month_holidays.columns = summary_month_holidays.columns.droplevel(0)\n",
    "    summary_month_holidays[\"mean\"]\n",
    "    print('Holidays that stock markets opening: ')\n",
    "    print(holidays)\n",
    "    def holidays(Dataset,title):\n",
    "        \"\"\"\n",
    "  \n",
    "        Explorae the holiday regularity of Dataset\n",
    "        \"\"\"\n",
    "        Dataset = Dataset.copy()\n",
    "        holidays = Dataset.loc[Dataset.index.isin(holidays_df['ds'])]\n",
    "        normaldays = Dataset.loc[~Dataset.index.isin(holidays_df['ds'])]\n",
    "        summary_month_holidays = holidays.groupby(holidays.index.month).describe()\n",
    "        summary_month_normaldays = normaldays.groupby(normaldays.index.month).describe()\n",
    "        summary_month_holidays.columns = summary_month_holidays.columns.droplevel(0)\n",
    "        summary_month_normaldays.columns = summary_month_normaldays.columns.droplevel(0)\n",
    "        f, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "        ax.plot(summary_month_holidays.index, summary_month_holidays.loc[:,'mean'], color='y', label='Holidays', ls='--', lw=3)\n",
    "        ax.fill_between(summary_month_holidays.index, summary_month_holidays.loc[:,'25%'], \\\n",
    "                        summary_month_holidays.loc[:,'75%'], facecolor='y', alpha=0.1)\n",
    "        ax.plot(summary_month_normaldays.index, summary_month_normaldays.loc[:,'mean'], color='b', label='Weekdays', lw=3)\n",
    "        ax.fill_between(summary_month_normaldays.index, summary_month_normaldays.loc[:,'25%'], \\\n",
    "                        summary_month_normaldays.loc[:,'75%'], facecolor='b', alpha=0.1)\n",
    "        ax.legend(fontsize=15)\n",
    "        ax.set_xticks(range(1,13));\n",
    "        ax.grid(ls=':', color='0.8')\n",
    "        ax.set_xlabel('Month', fontsize=15)\n",
    "        ax.set_ylabel(title, fontsize=15);\n",
    "\n",
    "        [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "        [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "        plt.savefig('./image/Data Exploration/holidays '+title)\n",
    "        plt.close()\n",
    "\n",
    "    holidays(Stock_data,'Price of stock')\n",
    "    holidays(Oil_data,'Price of Oil')\n",
    "    holidays(NASDAQ_data,'NASDAQ Index')\n",
    "    holidays(Precip_data,'Precipitation')\n",
    "    Y = Combine.loc[:,'Stock_Price']\n",
    "    Y1 = Combine.loc[:,'Oil_Price']\n",
    "    Y2 = Combine.loc[:,'NASDAQ_Index']\n",
    "    Y3 = Combine.loc[:,'New_cases']\n",
    "    Y4 = Combine.loc[:,'Precipitation']\n",
    "    image_dir='./image/Data Exploration/Data Relationships'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "    \"\"\"\n",
    "    Calculate the varianceof data,Covariance, correlation between data\n",
    "    \"\"\"\n",
    "    def DEmean(x):\n",
    "        x_mean = np.mean(x)\n",
    "        return [i - x_mean for i in x]\n",
    "\n",
    "    def covariance(x, y):\n",
    "        n = len(x)\n",
    "        return np.dot(DEmean(x), DEmean(y)) / (n-1)\n",
    "    var0=np.var(Y)\n",
    "    cov1=covariance(Y, Y1)\n",
    "    Cov1=np.cov(Y, Y1)\n",
    "    sn.heatmap(Cov1, annot=True, fmt='g')\n",
    "    plt.savefig('./image/Data Exploration/Data Relationships/Stock Oil')\n",
    "    plt.close()\n",
    "    var1=np.var(Y1)\n",
    "    corr1, _ = pearsonr(Y, Y1)\n",
    "    Corr1, _ = spearmanr(Y, Y1)\n",
    "    plt.scatter(Y, Y1)\n",
    "    plt.title('Stock Price  VS Curde Oil Price ')\n",
    "    plt.ylabel('Curde Oil Price ($USD$)')\n",
    "    plt.xlabel('Stock Price ($USD$)')\n",
    "    plt.savefig('./image/Data Exploration/Data Relationships/Stock Oil scatter')\n",
    "    plt.close()\n",
    "\n",
    "    cov2=covariance(Y, Y2)\n",
    "    Cov2=np.cov(Y, Y2)\n",
    "\n",
    "    sn.heatmap(Cov2, annot=True, fmt='g')\n",
    "    plt.savefig('./image/Data Exploration/Data Relationships/Stock NASDAQ')\n",
    "    plt.close()\n",
    "\n",
    "    var2=np.var(Y2)\n",
    "\n",
    "    corr2, _ = pearsonr(Y, Y2)\n",
    "\n",
    "\n",
    "    Corr2, _ = spearmanr(Y, Y2)\n",
    "\n",
    "\n",
    "    plt.scatter(Y, Y2)\n",
    "    plt.title('Stock Price  VS NASDAQ_Index ')\n",
    "    plt.ylabel('NASDAQ_Index ($USD$)')\n",
    "    plt.xlabel('Stock Price ($USD$)')\n",
    "    plt.savefig('./image/Data Exploration/Data Relationships/Stock NASDAQ scatter')\n",
    "    plt.close()\n",
    "\n",
    "    cov4=covariance(Y, Y4)\n",
    "    Cov4=np.cov(Y, Y4)\n",
    "\n",
    "    sn.heatmap(Cov4, annot=True, fmt='g')\n",
    "    plt.savefig('./image/Data Exploration/Data Relationships/Stock Precipitation')\n",
    "    plt.close()\n",
    "\n",
    "    var4=np.var(Y4)\n",
    "\n",
    "    corr4, _ = pearsonr(Y, Y4)\n",
    "    Corr4, _ = spearmanr(Y, Y4)\n",
    "    plt.scatter(Y, Y4)\n",
    "    plt.title('Stock Price  VS Precipitation ')\n",
    "    plt.ylabel('Precipitation ($Inch$)')\n",
    "    plt.xlabel('Stock Price ($USD$)')\n",
    "    plt.savefig('./image/Data Exploration/Data Relationships/Stock Precipitation scatter')\n",
    "    plt.close()\n",
    "    columns=['Oil_Price', 'NASDAQ_Index','Precipitation']\n",
    "    Oil_Price=[var1,cov1,corr1,Corr1]\n",
    "    NASDAQ_Index=[var2,cov2,corr2,Corr2]\n",
    "    Precipitation=[var4,cov4,corr4,Corr4]\n",
    "    index=['Variance','Covariance','Pearsonr_correlation','Spearmanr_correlation']\n",
    "\n",
    "    dic={'Oil_Price':Oil_Price, 'NASDAQ_Index':NASDAQ_Index, \n",
    "          'Precipitation':Precipitation}\n",
    "\n",
    "    df=pd.DataFrame(data=dic,index=index)\n",
    "    df['Stock_Price']=[var0,\"-\",\"-\",\"-\"]\n",
    "    print(df)\n",
    "    def Chi_Square_Stock_Price(tar1,tar2):\n",
    "        \"\"\"\n",
    "        Chi-Square test for stock price and other auxiliary Dataset\n",
    "        \"\"\"\n",
    "        thres1=1/3*(max(Combine[tar1])-min(Combine[tar1]))+min(Combine[tar1])\n",
    "        thres2=2/3*(max(Combine[(tar1)])-min(Combine[tar1]))+min(Combine[tar1])\n",
    "\n",
    "        stocklow = len(Combine[(Combine[tar1]<thres1)])\n",
    "        stockmid = len(Combine[(Combine[tar1]<thres2)&(Combine[tar1]>=thres1)])\n",
    "        stockhigh = len(Combine[(Combine[tar1]>=thres2)])\n",
    "\n",
    "        Thres1=1/3*(max(Combine[tar2])-min(Combine[tar2]))+min(Combine[tar2])\n",
    "        Thres2=2/3*(max(Combine[tar2])-min(Combine[tar2]))+min(Combine[tar2])\n",
    "        auxiliaryLow_Day=len(Combine[(Combine[tar2]<Thres1)])\n",
    "        auxiliaryMid_Day=len(Combine[(Combine[tar2]<Thres2)&(Combine[tar2]>=Thres1)])\n",
    "        auxiliaryHigh_Day=len(Combine[(Combine[tar2]>=Thres2)])\n",
    "\n",
    "        stocklow_auxiliaryLow = len(Combine[(Combine[tar2]<Thres1)&\n",
    "                             (Combine[tar1]<thres1)])\n",
    "        stocklow_auxiliaryMid = len(Combine[(Combine[tar2]<Thres2)&(Combine[tar2]>=Thres1)&\n",
    "                                            (Combine[tar1]<thres1)])\n",
    "        stocklow_auxiliaryHigh = len(Combine[(Combine[tar2]>=Thres2)&\n",
    "                              (Combine[tar1]<thres1)])\n",
    "\n",
    "        stockmid_auxiliaryLow = len(Combine[(Combine[tar2]<Thres1)&\n",
    "                                            (Combine[tar1]<thres2)&(Combine[tar1]>=thres1)])\n",
    "        stockmid_auxiliaryMid = len(Combine[(Combine[tar2]<Thres2)&(Combine[tar2]>=Thres1)&\n",
    "                                            (Combine[tar1]<thres2)&(Combine[tar1]>=thres1)])\n",
    "        stockmid_auxiliaryHigh = len(Combine[(Combine[tar2]>=Thres2)&\n",
    "                              (Combine[tar1]<thres2)&(Combine[tar1]>=thres1)])\n",
    "\n",
    "        stockhigh_auxiliaryLow = len(Combine[(Combine[tar2]<Thres1)&\n",
    "                              (Combine[tar1]>=thres2)])\n",
    "        stockhigh_auxiliaryMid = len(Combine[(Combine[tar2]<Thres2)&(Combine[tar2]>=Thres1)&\n",
    "                              (Combine[tar1]>=thres2)])\n",
    "        stockhigh_auxiliaryHigh = len(Combine[(Combine[tar2]>=Thres2)&\n",
    "                               (Combine[tar1]>=thres2)])\n",
    "\n",
    "        auxiliaryLow=[stocklow_auxiliaryLow,stockmid_auxiliaryLow,stockhigh_auxiliaryLow]  \n",
    "        auxiliaryMid=[stocklow_auxiliaryMid,stockmid_auxiliaryMid,stockhigh_auxiliaryMid] \n",
    "        auxiliaryHigh=[stocklow_auxiliaryHigh,stockmid_auxiliaryHigh,stockhigh_auxiliaryHigh]                                      \n",
    "\n",
    "        index=['stocklow','stockmid','stockhigh']\n",
    "        D={'auxiliaryLow':auxiliaryLow,'auxiliaryMid':auxiliaryMid,'auxiliaryHigh':auxiliaryHigh}\n",
    "        Table_Simple=pd.DataFrame(data=D,index=index,columns=None)   \n",
    "\n",
    "        index=['stocklow','stockmid','stockhigh','Total']\n",
    "        auxiliaryLow=[stocklow_auxiliaryLow,stockmid_auxiliaryLow,stockhigh_auxiliaryLow,auxiliaryLow_Day]  \n",
    "        auxiliaryMid=[stocklow_auxiliaryMid,stockmid_auxiliaryMid,stockhigh_auxiliaryMid,auxiliaryMid_Day] \n",
    "        auxiliaryHigh=[stocklow_auxiliaryHigh,stockmid_auxiliaryHigh,stockhigh_auxiliaryHigh,auxiliaryHigh_Day]\n",
    "\n",
    "        Total=[stocklow,stockmid,stockhigh,(stocklow+stockmid+stockhigh)]\n",
    "        D2={'auxiliaryLow':auxiliaryLow,'auxiliaryMid':auxiliaryMid,'auxiliaryHigh':auxiliaryHigh, 'Total': Total}\n",
    "        Table_with_Total = pd.DataFrame(data=D2,index=index,columns=None)\n",
    "        return (Table_Simple,Table_with_Total)\n",
    "    def Chi_Sqaure(CaliTable):\n",
    "        stat, p, dof, expected = chi2_contingency(CaliTable)\n",
    "        \"\"\"\n",
    "        Through Chi-Square test to judge dependency or independency\n",
    "        \"\"\"\n",
    "        print(\"statistic\",stat)\n",
    "        print(\"p-value\",p)\n",
    "        print(\"degres of fredom: \",dof)\n",
    "        print(\"table of expected frequencies\\n\",expected)\n",
    "        prob = 0.90\n",
    "        critical = chi2.ppf(prob, dof)\n",
    "        if abs(stat) >= critical:\n",
    "            print('Dependent (reject H0)')\n",
    "        else:\n",
    "            print('Independent (fail to reject H0)')\n",
    "\n",
    "    OilTable, OilTable_with_Total=Chi_Square_Stock_Price('Stock_Price','Oil_Price')\n",
    "    Chi_Sqaure(OilTable)\n",
    "\n",
    "    NASDAQTable, NASDAQTable_with_Total=Chi_Square_Stock_Price('Stock_Price','NASDAQ_Index')\n",
    "    Chi_Sqaure(NASDAQTable)\n",
    "\n",
    "    PrecipitationTable, PrecipitationTable_with_Total=Chi_Square_Stock_Price('Stock_Price','Precipitation')\n",
    "    Chi_Sqaure(PrecipitationTable)\n",
    "\n",
    "    def Chi_Square_Stock_Price_for_Pandemic(tar1,tar2):\n",
    "        \"\"\"\n",
    "        Chi-Square test for stock price, other auxiliary Dataset and pandemic conditions\n",
    "        \"\"\"\n",
    "        thres1=1/3*(max(Combine[tar1])-min(Combine[tar1]))+min(Combine[tar1])\n",
    "        thres2=2/3*(max(Combine[(tar1)])-min(Combine[tar1]))+min(Combine[tar1])\n",
    "\n",
    "        stocklow = len(Combine[(Combine[tar1]<thres1)])\n",
    "        stockmid = len(Combine[(Combine[tar1]<thres2)&(Combine[tar1]>=thres1)])\n",
    "        stockhigh = len(Combine[(Combine[tar1]>=thres2)])\n",
    "\n",
    "        Pandemic_Day=len(Combine[(Combine[tar2]==True)])\n",
    "        NoPandemic_Day=len(Combine[(Combine[tar2]==False)])\n",
    "\n",
    "        stocklow_Pandemic = len(Combine[(Combine[tar2]==True)&\n",
    "                             (Combine[tar1]<thres1)])\n",
    "        stocklow_NoPandemic = len(Combine[(Combine[tar2]==False)&\n",
    "                                            (Combine[tar1]<thres1)])\n",
    "\n",
    "        stockmid_Pandemic = len(Combine[(Combine[tar2]==True)&\n",
    "                                            (Combine[tar1]<thres2)&(Combine[tar1]>=thres1)])\n",
    "        stockmid_NoPandemic = len(Combine[(Combine[tar2]==False)&\n",
    "                                            (Combine[tar1]<thres2)&(Combine[tar1]>=thres1)])\n",
    "\n",
    "        stockhigh_Pandemic = len(Combine[(Combine[tar2]==True)&\n",
    "                              (Combine[tar1]>=thres2)])\n",
    "        stockhigh_NoPandemic = len(Combine[(Combine[tar2]==False)&\n",
    "                              (Combine[tar1]>=thres2)])\n",
    "\n",
    "        Pandemic=[stocklow_Pandemic,stockmid_Pandemic,stockhigh_Pandemic]  \n",
    "        NoPandemic=[stocklow_NoPandemic,stockmid_NoPandemic,stockhigh_NoPandemic] \n",
    "\n",
    "\n",
    "        index=['stocklow','stockmid','stockhigh']\n",
    "        D={'Pandemic':Pandemic,'NoPandemic':NoPandemic}\n",
    "        Table_Simple=pd.DataFrame(data=D,index=index,columns=None)   \n",
    "\n",
    "        index=['stocklow','stockmid','stockhigh','Total']\n",
    "        Pandemic=[stocklow_Pandemic,stockmid_Pandemic,stockhigh_Pandemic,Pandemic_Day]  \n",
    "        NoPandemic=[stocklow_NoPandemic,stockmid_NoPandemic,stockhigh_NoPandemic,NoPandemic_Day] \n",
    "\n",
    "        Total=[stocklow,stockmid,stockhigh,(stocklow+stockmid+stockhigh)]\n",
    "        D2={'Pandemic':Pandemic,'NoPandemic':NoPandemic, 'Total': Total}\n",
    "        Table_with_Total = pd.DataFrame(data=D2,index=index,columns=None)\n",
    "        return (Table_Simple,Table_with_Total)\n",
    "    casesTable, casesTable_with_Total=Chi_Square_Stock_Price_for_Pandemic('Stock_Price','New_cases')\n",
    "    Chi_Sqaure(casesTable)\n",
    "    print('task4 succeed')\n",
    "    return holidays_df # holiday data is useful for follows inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374f0c7-95dd-4978-9612-2c45396b0dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3de2cc9-5f92-48b5-aa99-2c12dbf74990",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "\n",
    "## Task 5: Inference\n",
    "\n",
    "Train a model to predict the closing stock price on each day for the data you have already\n",
    "collected, stored, preprocessed and explored from previous steps. The data must be spanning\n",
    "from April 2017 to April 202 1.\n",
    "You should develop two separate models:\n",
    "\n",
    "\n",
    "1. A model for predicting the closing stock price on each day for a 1-month time window (until\n",
    "    end of May 202 1 ), using only time series of stock prices.\n",
    "2. A model for predicting the closing stock price on each day for a 1-month time window (until\n",
    "    end of May 202 1 ), using the time series of stock prices and the auxiliary data you collected.\n",
    "Which model is performing better? How do you measure performance and why? How could you\n",
    "further improve the performance? Are the models capable of predicting the closing stock prices\n",
    "far into the future?\n",
    "\n",
    "[IMPORTANT NOTE] For these tasks, you are not expected to compare model architectures, but\n",
    "examine and analyse the differences when training the same model with multiple data attributes\n",
    "and information from sources. Therefore, you should decide a single model suitable for time series\n",
    "data to solve the tasks described above. Please see the lecture slides for tips on model selection\n",
    "and feel free to experiment before selecting one.\n",
    "\n",
    "The following would help you evaluate your approach and highlight potential weaknesses in your\n",
    "process:\n",
    "\n",
    "1. Evaluate the performance of your model using different metrics, e.g. mean squared error,\n",
    "    mean absolute error or R-squared.\n",
    "2. Use ARIMA and Facebook Prophet to explore the uncertainty on your model’s predicted\n",
    "    values by employing confidence bands.\n",
    "3. Result visualization: create joint plots showing marginal distributions to understand the\n",
    "    correlation between actual and predicted values.\n",
    "4. Finding the mean, median and skewness of the residual distribution might provide\n",
    "    additional insight into the predictive capability of the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d85b117-1d3e-46a0-ab2b-a031b0d05b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Stock_fit,NASDAQ_fit,Combine,Combine_fit,holidays_df):\n",
    "    \"\"\"\n",
    "    Train a Facebook Prophet model\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import seaborn as sn\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from scipy.stats import pearsonr,spearmanr,chi2, chi2_contingency\n",
    "    from calendar import day_abbr, month_abbr, mdays\n",
    "    import holidays\n",
    "    from fbprophet import Prophet\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "    image_dir='./image/Inference/single'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "    def train_test_split(Data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Split data into train set and test set\n",
    "        \"\"\"\n",
    "        data = Data.copy()\n",
    "        data.reset_index(inplace=True)    \n",
    "        df=data.rename(columns={'Date':'ds','Stock_Price':'y'})\n",
    "\n",
    "        train = df.set_index('ds').loc[:'2021-04-30', :].reset_index()\n",
    "        test = df.set_index('ds').loc['2021-05-03':, :].reset_index()\n",
    "        return train,test\n",
    "\n",
    "    def fit_model(train):   \n",
    "        \"\"\"Add condition and data in model.\"\"\"\n",
    "        m = Prophet(\n",
    "            holidays=holidays_df,\n",
    "    #         changepoint_prior_scale=0.05, \n",
    "            seasonality_mode='multiplicative',\n",
    "            daily_seasonality=False,\n",
    "            yearly_seasonality=True, \n",
    "            weekly_seasonality=True, \n",
    "    #        growth=\"logistic\",\n",
    "        )\n",
    "\n",
    "    #     m.add_seasonality(name='monthly', period=30.5, fourier_order=5, prior_scale=0.1)\n",
    "    #     m.add_country_holidays(country_name='US')   \n",
    "\n",
    "        m.fit(train)\n",
    "        return m\n",
    "\n",
    "\n",
    "    import json\n",
    "    from fbprophet.serialize import model_to_json, model_from_json\n",
    "\n",
    "    def save_model(filename):\n",
    "        \"\"\"\n",
    "        Function to save model to local disk\n",
    "        \"\"\"\n",
    "        model_dir='./model'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)    \n",
    "        file_path=model_dir+'/'+filename+'.json'\n",
    "        with open(file_path, 'w') as fout:\n",
    "            json.dump(model_to_json(m), fout)  # Save model\n",
    "\n",
    "    def load_model(filename):\n",
    "        \"\"\"\n",
    "        Function to load model from local disk\n",
    "        \"\"\"\n",
    "        model_dir='./model'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)    \n",
    "        file_path=model_dir+'/'+filename+'.json'\n",
    "        with open(file_path, 'r') as fin:\n",
    "            m = model_from_json(json.load(fin))  # Load model\n",
    "        return m\n",
    "\n",
    "    train,test=train_test_split(Stock_fit)\n",
    "    m = fit_model(train)\n",
    "    save_model('Single_model')\n",
    "    def Stock_with_auxiliary(data_fit,title):\n",
    "        \"\"\"\n",
    "        Build a model using stock price and auxiliary data\n",
    "        \"\"\"\n",
    "        \n",
    "        image_dir='./image/Inference/'+title\n",
    "        if not os.path.exists(image_dir):\n",
    "            os.makedirs(image_dir)\n",
    "        Stock_N=pd.concat([Stock_fit,data_fit],axis=1)\n",
    "        train_N, test_N=train_test_split(Stock_N)\n",
    "\n",
    "        m = Prophet(holidays=holidays_df, \n",
    "                    seasonality_mode='multiplicative',\n",
    "                    yearly_seasonality=True, \n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False)\n",
    "        m.add_regressor(title, mode='multiplicative')\n",
    "\n",
    "        m.fit(train_N)\n",
    "\n",
    "        future = m.make_future_dataframe(periods=30, freq='1D')\n",
    "        future=future.set_index('ds')\n",
    "        futures = pd.concat([future, data_fit.loc[:, [title]]], axis=1)\n",
    "        futures.reset_index(inplace=True)\n",
    "        futures.rename(columns={'index':'ds'},inplace=True)\n",
    "        futures =futures.loc[futures['ds'].dt.weekday.isin([0, 1, 2, 3, 4])]\n",
    "        save_model(title+'_model')\n",
    "        return m,train_N, test_N,futures\n",
    "\n",
    "    def Stock_with_auxiliary_with_Pandemic(data_fit1,title):\n",
    "        \"\"\"\n",
    "        Build a model using stock price,auxiliary data and pandemic condition.\n",
    "        \"\"\"\n",
    "        image_dir='./image/Inference/with_Pandemic/'+title\n",
    "        if not os.path.exists(image_dir):\n",
    "            os.makedirs(image_dir)\n",
    "        data_fit2=Combine['New_cases']\n",
    "        Stock_N=pd.concat([Stock_fit,data_fit1,data_fit2],axis=1)\n",
    "        Stock_N=Stock_N.rename(columns={'New_cases':'Pandemic'})\n",
    "        Stock_N['NoPandemic']=~data_fit2\n",
    "        train_N, test_N=train_test_split(Stock_N)\n",
    "\n",
    "        m = Prophet(holidays=holidays_df, \n",
    "                    seasonality_mode='multiplicative',\n",
    "                    yearly_seasonality=True, \n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False)\n",
    "        \"\"\"\n",
    "        Add auxiliary data in to model, so that it can training\n",
    "        \"\"\"\n",
    "        m.add_regressor(title, mode='multiplicative')\n",
    "        m.add_seasonality(name='Pandemic', period=365, fourier_order=3, mode='multiplicative', condition_name='Pandemic')\n",
    "        m.add_seasonality(name='NoPandemic', period=365, fourier_order=3, mode='multiplicative', condition_name='NoPandemic')\n",
    "        m.fit(train_N)\n",
    "        future = m.make_future_dataframe(periods=30, freq='1D')\n",
    "        future=future.set_index('ds')\n",
    "        futures = pd.concat([future, data_fit1.loc[:, [title]],Stock_N['Pandemic'],Stock_N['NoPandemic']], axis=1)\n",
    "        futures.reset_index(inplace=True)\n",
    "        futures.rename(columns={'index':'ds'},inplace=True)\n",
    "        futures =futures.loc[futures['ds'].dt.weekday.isin([0, 1, 2, 3, 4])]\n",
    "        save_model(title+'_Pandemic_model')\n",
    "        return m,train_N, test_N,futures\n",
    "\n",
    "    \n",
    "#     model1,tra1,te1,fu1=Stock_with_auxiliary(Oil_fit,'Oil_Price')\n",
    "#     Model1,Tra1, Te1,Fu1=Stock_with_auxiliary_with_Pandemic(Oil_fit,'Oil_Price')\n",
    "\n",
    "    model2,tra2,te2,fu2=Stock_with_auxiliary(NASDAQ_fit,'NASDAQ_Index')\n",
    "    Model2,Tra2, Te2,Fu2=Stock_with_auxiliary_with_Pandemic(NASDAQ_fit,'NASDAQ_Index')\n",
    "\n",
    "#     model3,tra3,te3,fu3=Stock_with_auxiliary(Precip_fit,'Precipitation')\n",
    "#     Model3,Tra3, Te3,Fu3=Stock_with_auxiliary_with_Pandemic(Precip_fit,'Precipitation')\n",
    "#     modelA,traA,teA,fuA=Stock_with_all_auxiliary(\"All\")\n",
    "#     ModelA,TraA, TeA,FuA=Stock_with_all_auxiliary_with_Pandemic(\"All\")\n",
    "\n",
    "\n",
    "    print('task5 train succeed')\n",
    "    return m,train,test,model2,tra2,te2,fu2,Model2,Tra2, Te2,Fu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c3918-fc22-4609-a3f8-3d39deffae90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e30e9e7-b431-4e7b-9d2d-e5495e3ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Stock_data,m,train,test,model2,tra2,te2,fu2,Model2,Tra2, Te2,Fu2,scaler0):\n",
    "    \"\"\"test and evaluate model\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import seaborn as sn\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from scipy.stats import pearsonr,spearmanr,chi2, chi2_contingency\n",
    "    from calendar import day_abbr, month_abbr, mdays\n",
    "    import holidays\n",
    "    from fbprophet import Prophet\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "    \n",
    "    def make_predict(m,periods):    \n",
    "        \"\"\"\n",
    "        Add auxiliary data in the table to prepare for test\n",
    "        \"\"\"\n",
    "        future = m.make_future_dataframe(periods=periods, freq='1D')\n",
    "        future=future.loc[future['ds'].dt.weekday.isin([0, 1, 2, 3, 4])]\n",
    "        forecast = m.predict(future)   \n",
    "        fig = m.plot_components(forecast, figsize=(12, 16))\n",
    "\n",
    "        return forecast,future\n",
    "\n",
    "    def make_predictions_df(forecast, data_train, data_test): \n",
    "        \"\"\"\n",
    "        Function to convert the output Prophet dataframe to timestamp and append the actual target values at the end\n",
    "        \"\"\"\n",
    "        forecast.index = pd.to_datetime(forecast.ds)\n",
    "        data_train.index = pd.to_datetime(data_train.ds)\n",
    "        data_test.index = pd.to_datetime(data_test.ds)\n",
    "        data = pd.concat([data_train, data_test], axis=0)\n",
    "        forecast.loc[:,'y'] = data.loc[:,'y']\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    def plot_predictions(forecast, start_date):\n",
    "        \"\"\"\n",
    "        Function to plot the predictions \n",
    "        \"\"\"\n",
    "        f, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "        train = forecast.loc[start_date:'2021-04-30',:]\n",
    "        ax.plot(train.index, train.y, 'ko', markersize=3)\n",
    "        ax.plot(train.index, train.yhat, color='steelblue', lw=0.5)\n",
    "        ax.fill_between(train.index, train.yhat_lower, train.yhat_upper, color='steelblue', alpha=0.3)\n",
    "\n",
    "        test = forecast.loc['2021-05-03':,:]\n",
    "        ax.plot(test.index, test.y, 'ro', markersize=3)\n",
    "        ax.plot(test.index, test.yhat, color='coral', lw=0.5)\n",
    "        ax.fill_between(test.index, test.yhat_lower, test.yhat_upper, color='coral', alpha=0.3)\n",
    "        ax.axvline(forecast.loc['2021-05-03', 'ds'], color='k', ls='--', alpha=0.7)\n",
    "        ax.grid(ls=':', lw=0.5)\n",
    "        return f, ax\n",
    "    \n",
    "    def create_joint_plot(forecast, x='yhat', y='y', title=None): \n",
    "        \"\"\"\n",
    "        Function to plot joint diagram for train set\n",
    "        \"\"\"\n",
    "\n",
    "        g = sn.jointplot(x='yhat', y='y', data=forecast, kind=\"reg\", color=\"b\")\n",
    "        g.fig.set_figwidth(8)\n",
    "        g.fig.set_figheight(8)\n",
    "\n",
    "        ax = g.fig.axes[1]\n",
    "        if title is not None: \n",
    "            ax.set_title(title, fontsize=16)\n",
    "\n",
    "        ax = g.fig.axes[0]\n",
    "        ax.text(0.2, 0.8, \"R = {:+4.2f}\".format(forecast.loc[:,['y','yhat']].corr().iloc[0,1]), fontsize=16)\n",
    "        R=forecast.loc[:,['y','yhat']].corr().iloc[0,1]\n",
    "        ax.set_xlabel('Predictions', fontsize=15)\n",
    "        ax.set_ylabel('Observations', fontsize=15)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(ls=':')\n",
    "        [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "        [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()];\n",
    "\n",
    "        ax.grid(ls=':')\n",
    "        return R\n",
    "    \n",
    "    forecast,future=make_predict(m,30)\n",
    "    plt.close()\n",
    "    m.plot_components(forecast).savefig('./image/Inference/single/stock forecast')\n",
    "    plt.close()\n",
    "    result = make_predictions_df(forecast, train, test)\n",
    "    result.loc[:,'yhat'] = result.yhat.clip(lower=0)\n",
    "    result.loc[:,'yhat_lower'] = result.yhat_lower.clip(lower=0)\n",
    "    result.loc[:, 'yhat_upper'] = result.yhat_upper.clip(lower=0)\n",
    "    result.head()\n",
    "\n",
    "    f, ax = plot_predictions(result, '2017-04-03')\n",
    "    plt.savefig('./image/Inference/single/stock')\n",
    "    plt.close()\n",
    "    r0=create_joint_plot(result.loc[:'2021-4-30', :], title='Train set')\n",
    "    plt.savefig('./image/Inference/single/stock Train set')\n",
    "    plt.close()\n",
    "    rt0=create_joint_plot(result.loc['2021-05-03':, :], title='Test set')\n",
    "    plt.savefig('./image/Inference/single/stock Test set')\n",
    "    plt.close()\n",
    "    \"\"\"\n",
    "    Get the predict value of model using only stock price\n",
    "    \"\"\"\n",
    "    predict0 = pd.DataFrame({\n",
    "        'True_Value':Stock_data.loc['2021-05-03':,'Stock_Price'],\n",
    "        'Predict': result.loc['2021-05-03':,'yhat']\n",
    "        })\n",
    "    predict0.loc[:,'Predict']=scaler0.inverse_transform(pd.DataFrame(predict0.loc['2021-05-03':,'Predict']))\n",
    "    \n",
    "    def pred1(m,train_N, test_N,futures,title):\n",
    "        \"\"\"\n",
    "        Get the predict value of model using stock price and auxiliary data\n",
    "        \"\"\"\n",
    "        forecast = m.predict(futures)\n",
    "        f = m.plot_components(forecast, figsize=(12, 16))\n",
    "        plt.close()\n",
    "        m.plot_components(forecast).savefig('./image/Inference/'+title+'/forecast')\n",
    "        plt.close()\n",
    "        result = make_predictions_df(forecast, train_N, test_N)\n",
    "        result.loc[:,'yhat'] = result.yhat.clip(lower=0)\n",
    "        result.loc[:,'yhat_lower'] = result.yhat_lower.clip(lower=0)\n",
    "        result.loc[:, 'yhat_upper'] = result.yhat_upper.clip(lower=0)\n",
    "        result.head()\n",
    "\n",
    "        f, ax = plot_predictions(result, '2017-04-03')\n",
    "        plt.savefig('./image/Inference/'+title+'/predict')\n",
    "        plt.close()\n",
    "        R_train = create_joint_plot(result.loc[:'2021-4-30', :], title='Train set')\n",
    "        plt.savefig('./image/Inference/'+title+'/Train set')\n",
    "        plt.close()\n",
    "        R_test = create_joint_plot(result.loc['2021-05-03':, :], title='Test set')\n",
    "        plt.savefig('./image/Inference/'+title+'/Test set')\n",
    "        plt.close()\n",
    "\n",
    "        predict = pd.DataFrame({\n",
    "            'True_Value':Stock_data.loc['2021-05-03':,'Stock_Price'],\n",
    "            'Predict': result.loc['2021-05-03':,'yhat']\n",
    "            })\n",
    "        predict.loc[:,'Predict']=scaler0.inverse_transform(pd.DataFrame(predict.loc['2021-05-03':,'Predict']))\n",
    "        return predict,R_train,R_test\n",
    "    def pred2(m,train_N, test_N,futures,title):\n",
    "        \"\"\"\n",
    "        Get the predict value of model using stock price,auxiliary data and pandemic condition..\n",
    "        \"\"\"      \n",
    "        forecast = m.predict(futures)\n",
    "\n",
    "        f = m.plot_components(forecast, figsize=(12, 16))\n",
    "        plt.close()\n",
    "        m.plot_components(forecast).savefig('./image/Inference/with_Pandemic/'+title+'/forecast')\n",
    "        plt.close()\n",
    "        result = make_predictions_df(forecast, train_N, test_N)\n",
    "        result.loc[:,'yhat'] = result.yhat.clip(lower=0)\n",
    "        result.loc[:,'yhat_lower'] = result.yhat_lower.clip(lower=0)\n",
    "        result.loc[:, 'yhat_upper'] = result.yhat_upper.clip(lower=0)\n",
    "        result.head()\n",
    "\n",
    "        f, ax = plot_predictions(result, '2017-04-03')\n",
    "        plt.savefig('./image/Inference/with_Pandemic/'+title+'/predic')\n",
    "        plt.close()\n",
    "        R_train=create_joint_plot(result.loc[:'2021-4-30', :], title='Train set')\n",
    "        plt.savefig('./image/Inference/with_Pandemic/'+title+'/Train set')\n",
    "        plt.close()\n",
    "        R_test=create_joint_plot(result.loc['2021-05-03':, :], title='Test set')\n",
    "        plt.savefig('./image/Inference/with_Pandemic/'+title+'/Test set')\n",
    "        plt.close()\n",
    "\n",
    "        predict = pd.DataFrame({\n",
    "            'True_Value':Stock_data.loc['2021-05-03':,'Stock_Price'],\n",
    "            'Predict': result.loc['2021-05-03':,'yhat']\n",
    "            })\n",
    "        predict.loc[:,'Predict']=scaler0.inverse_transform(pd.DataFrame(predict.loc['2021-05-03':,'Predict']))\n",
    "        return predict,R_train,R_test\n",
    "\n",
    "\n",
    "#     predict1,r1,rt1=pred1(model1,tra1,te1,fu1,'Oil_Price')\n",
    "#     Predict1,R1,RT1=pred2(Model1,Tra1, Te1,Fu1,'Oil_Price')\n",
    "\n",
    "    predict2,r2,rt2=pred1(model2,tra2,te2,fu2,'NASDAQ_Index')\n",
    "    Predict2,R2,RT2=pred2(Model2,Tra2, Te2,Fu2,'NASDAQ_Index')\n",
    "\n",
    "\n",
    "#     predict3,r3,rt3=pred1(model3,tra3,te3,fu3,'Precipitation')\n",
    "#     Predict3,R3,RT3=pred2(Model3,Tra3, Te3,Fu3,'Precipitation')\n",
    "    \n",
    "#     predictA,rA,rtA=pred3(modelA,traA,teA,fuA,\"All\")\n",
    "#     PredictA,RA,RTA=pred4(ModelA,TraA, TeA,FuA,\"All\")\n",
    "    Predict_table=predict0.copy()\n",
    "    Predict_table['N_predict']=predict2['Predict']\n",
    "    Predict_table['N_predict_PAN']=Predict2['Predict']\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaulate data through MSE model and R_Score testing.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "    def MSE(predict):\n",
    "        mse=mean_squared_error(predict['True_Value'],predict['Predict'])\n",
    "        return mse\n",
    "    \n",
    "    from sklearn.metrics import r2_score\n",
    "    def R_sqaure(predict):\n",
    "        return r2_score(predict['True_Value'],predict['Predict'])\n",
    "    index=['stock','with NASDAQ','with NASDAQ_PAN']\n",
    "    correlation_train=[r0,r2,R2]\n",
    "    correlation_test=[rt0,rt2,RT2]\n",
    "    MSE_result=[MSE(predict0),MSE(predict2),MSE(Predict2)]\n",
    "    R_sqaure_result=[R_sqaure(predict0),R_sqaure(predict2),R_sqaure(Predict2)]\n",
    "    Result_table={'correlation_train':correlation_train,'correlation_test':correlation_test,'MSE_result':MSE_result,'R_sqaure_result':R_sqaure_result}\n",
    "    Df=pd.DataFrame(Result_table,index=index)\n",
    "    print('task5 evaluate succeed')\n",
    "    return Predict_table,Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabd1f2-e948-4c4f-8c7d-b175a886c953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b389aa-b4b3-47a1-8893-e24d38a60f9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\">\n",
    "\n",
    "## Autorun\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfab70d1-6422-4441-a038-2a963f7158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    Stock,Oil, NASDAQ,caseUS,Precip=acquire()\n",
    "    \n",
    "    StockPrices,Oil_db,NASDAQ_db, Infects, Precip_db=store(Stock,Oil, NASDAQ,caseUS,Precip)\n",
    "    \n",
    "    Stock_data,Oil_data, NASDAQ_data,Precip_data,Combine,Stock_fit,Oil_fit,NASDAQ_fit,Precip_fit,Combine_fit,scaler0=process(StockPrices,Oil_db,NASDAQ_db, Infects, Precip_db)\n",
    "   \n",
    "    holidays_df =explore(Stock_data,Oil_data, NASDAQ_data,Precip_data,Combine,Stock_fit,Oil_fit,NASDAQ_fit,Precip_fit,Combine_fit)\n",
    "    \n",
    "    m,Train,Test,model2,tra2,te2,fu2,Model2,Tra2, Te2,Fu2=train(Stock_fit,NASDAQ_fit,Combine,Combine_fit,holidays_df)\n",
    "    \n",
    "    Pre_table, result=evaluate(Stock_data, m,Train,Test,model2,tra2,te2,fu2,Model2,Tra2, Te2,Fu2,scaler0)\n",
    "    \n",
    "    return Pre_table,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49f25b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1 succeed\n",
      "task2 succeed\n",
      "Wheher the missing value exists:\n",
      "Stock_Price      False\n",
      "Oil_Price        False\n",
      "NASDAQ_Index     False\n",
      "Precipitation    False\n",
      "New_cases        False\n",
      "dtype: bool\n",
      "task3 succeed\n",
      "Holidays that stock markets opening: \n",
      "            Stock_Price\n",
      "Date                   \n",
      "2017-10-09    50.599998\n",
      "2017-11-10    45.820000\n",
      "2018-10-08    35.900002\n",
      "2018-11-12    36.860001\n",
      "2019-10-14    27.620001\n",
      "2019-11-11    30.590000\n",
      "2020-10-12    12.920000\n",
      "2020-11-11    12.040000\n",
      "                        Oil_Price  NASDAQ_Index  Precipitation Stock_Price\n",
      "Variance               129.861042  4.902779e+06       0.209567   181.71048\n",
      "Covariance              80.450813 -2.164478e+04       1.064298           -\n",
      "Pearsonr_correlation     0.523222 -7.244811e-01       0.172305           -\n",
      "Spearmanr_correlation    0.449956 -8.122050e-01       0.222894           -\n",
      "statistic 214.16169905703848\n",
      "p-value 3.3816346269019644e-45\n",
      "degres of fredom:  4\n",
      "table of expected frequencies\n",
      " [[ 24.36103152 165.21776504 137.42120344]\n",
      " [ 31.14040115 211.19579752 175.66380134]\n",
      " [ 22.49856734 152.58643744 126.91499522]]\n",
      "Dependent (reject H0)\n",
      "statistic 708.693611678762\n",
      "p-value 4.568671791240487e-152\n",
      "degres of fredom:  4\n",
      "table of expected frequencies\n",
      " [[220.49856734  54.96848138  51.53295129]\n",
      " [281.86055396  70.26552053  65.8739255 ]\n",
      " [203.6408787   50.76599809  47.59312321]]\n",
      "Dependent (reject H0)\n",
      "statistic 151.03672795427448\n",
      "p-value 1.2205537749996283e-31\n",
      "degres of fredom:  4\n",
      "table of expected frequencies\n",
      " [[ 59.34097421 158.34670487 109.31232092]\n",
      " [ 75.8548233  202.41260745 139.73256925]\n",
      " [ 54.80420248 146.24068768 100.95510984]]\n",
      "Dependent (reject H0)\n",
      "statistic 876.8208713288897\n",
      "p-value 3.988108655809491e-191\n",
      "degres of fredom:  2\n",
      "table of expected frequencies\n",
      " [[110.56160458 216.43839542]\n",
      " [141.32951289 276.67048711]\n",
      " [102.10888252 199.89111748]]\n",
      "Dependent (reject H0)\n",
      "task4 succeed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task5 train succeed\n",
      "task5 evaluate succeed\n"
     ]
    }
   ],
   "source": [
    "Pre_table,result=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5136aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True_Value</th>\n",
       "      <th>Predict</th>\n",
       "      <th>N_predict</th>\n",
       "      <th>N_predict_PAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-03</th>\n",
       "      <td>21.950001</td>\n",
       "      <td>21.934342</td>\n",
       "      <td>22.077118</td>\n",
       "      <td>19.678055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-04</th>\n",
       "      <td>21.420000</td>\n",
       "      <td>21.940400</td>\n",
       "      <td>21.785593</td>\n",
       "      <td>19.262819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-05</th>\n",
       "      <td>21.570000</td>\n",
       "      <td>21.905665</td>\n",
       "      <td>21.743741</td>\n",
       "      <td>19.220425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-06</th>\n",
       "      <td>21.490000</td>\n",
       "      <td>21.856712</td>\n",
       "      <td>21.816173</td>\n",
       "      <td>19.353246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-07</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.833850</td>\n",
       "      <td>21.969279</td>\n",
       "      <td>19.613476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-10</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.776751</td>\n",
       "      <td>21.633950</td>\n",
       "      <td>19.288033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-11</th>\n",
       "      <td>21.570000</td>\n",
       "      <td>21.800617</td>\n",
       "      <td>21.644801</td>\n",
       "      <td>19.378144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-12</th>\n",
       "      <td>20.760000</td>\n",
       "      <td>21.785726</td>\n",
       "      <td>21.244461</td>\n",
       "      <td>18.881194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-13</th>\n",
       "      <td>21.209999</td>\n",
       "      <td>21.758907</td>\n",
       "      <td>21.379690</td>\n",
       "      <td>19.176620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-14</th>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.760881</td>\n",
       "      <td>21.769614</td>\n",
       "      <td>19.856249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-17</th>\n",
       "      <td>23.350000</td>\n",
       "      <td>21.788974</td>\n",
       "      <td>21.824836</td>\n",
       "      <td>20.327697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-18</th>\n",
       "      <td>23.559999</td>\n",
       "      <td>21.844094</td>\n",
       "      <td>21.770018</td>\n",
       "      <td>20.400827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-19</th>\n",
       "      <td>22.969999</td>\n",
       "      <td>21.860156</td>\n",
       "      <td>21.804680</td>\n",
       "      <td>20.619278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-20</th>\n",
       "      <td>22.600000</td>\n",
       "      <td>21.863902</td>\n",
       "      <td>22.126573</td>\n",
       "      <td>21.271446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-21</th>\n",
       "      <td>22.570000</td>\n",
       "      <td>21.896160</td>\n",
       "      <td>22.078671</td>\n",
       "      <td>21.394235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-24</th>\n",
       "      <td>22.990000</td>\n",
       "      <td>22.005575</td>\n",
       "      <td>22.447529</td>\n",
       "      <td>22.580166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-25</th>\n",
       "      <td>23.209999</td>\n",
       "      <td>22.083453</td>\n",
       "      <td>22.482523</td>\n",
       "      <td>22.870744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-26</th>\n",
       "      <td>23.900000</td>\n",
       "      <td>22.118311</td>\n",
       "      <td>22.623621</td>\n",
       "      <td>23.332206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-27</th>\n",
       "      <td>24.430000</td>\n",
       "      <td>22.136885</td>\n",
       "      <td>22.654435</td>\n",
       "      <td>23.643856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-28</th>\n",
       "      <td>24.240000</td>\n",
       "      <td>22.180271</td>\n",
       "      <td>22.698734</td>\n",
       "      <td>23.988235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            True_Value    Predict  N_predict  N_predict_PAN\n",
       "2021-05-03   21.950001  21.934342  22.077118      19.678055\n",
       "2021-05-04   21.420000  21.940400  21.785593      19.262819\n",
       "2021-05-05   21.570000  21.905665  21.743741      19.220425\n",
       "2021-05-06   21.490000  21.856712  21.816173      19.353246\n",
       "2021-05-07   22.000000  21.833850  21.969279      19.613476\n",
       "2021-05-10   22.000000  21.776751  21.633950      19.288033\n",
       "2021-05-11   21.570000  21.800617  21.644801      19.378144\n",
       "2021-05-12   20.760000  21.785726  21.244461      18.881194\n",
       "2021-05-13   21.209999  21.758907  21.379690      19.176620\n",
       "2021-05-14   22.400000  21.760881  21.769614      19.856249\n",
       "2021-05-17   23.350000  21.788974  21.824836      20.327697\n",
       "2021-05-18   23.559999  21.844094  21.770018      20.400827\n",
       "2021-05-19   22.969999  21.860156  21.804680      20.619278\n",
       "2021-05-20   22.600000  21.863902  22.126573      21.271446\n",
       "2021-05-21   22.570000  21.896160  22.078671      21.394235\n",
       "2021-05-24   22.990000  22.005575  22.447529      22.580166\n",
       "2021-05-25   23.209999  22.083453  22.482523      22.870744\n",
       "2021-05-26   23.900000  22.118311  22.623621      23.332206\n",
       "2021-05-27   24.430000  22.136885  22.654435      23.643856\n",
       "2021-05-28   24.240000  22.180271  22.698734      23.988235"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pre_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10762ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlation_train</th>\n",
       "      <th>correlation_test</th>\n",
       "      <th>MSE_result</th>\n",
       "      <th>R_sqaure_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stock</th>\n",
       "      <td>0.991097</td>\n",
       "      <td>0.688185</td>\n",
       "      <td>1.246649</td>\n",
       "      <td>-0.189704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with NASDAQ</th>\n",
       "      <td>0.991928</td>\n",
       "      <td>0.810072</td>\n",
       "      <td>0.821112</td>\n",
       "      <td>0.216395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with NASDAQ_PAN</th>\n",
       "      <td>0.992023</td>\n",
       "      <td>0.877136</td>\n",
       "      <td>4.048300</td>\n",
       "      <td>-2.863381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 correlation_train  correlation_test  MSE_result  \\\n",
       "stock                     0.991097          0.688185    1.246649   \n",
       "with NASDAQ               0.991928          0.810072    0.821112   \n",
       "with NASDAQ_PAN           0.992023          0.877136    4.048300   \n",
       "\n",
       "                 R_sqaure_result  \n",
       "stock                  -0.189704  \n",
       "with NASDAQ             0.216395  \n",
       "with NASDAQ_PAN        -2.863381  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b462e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
